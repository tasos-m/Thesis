{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA models for different topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, LDA models are trained on the corresponding input that every use case indicates, with number of topics indicated by the best performing models from the lda_select_model.ipynb. (based on the topic coherence score). Then the testing project is inserted to the trained LDA models and is assigned to the topics, so we can find its dominant one. Following that, we present the other documents with the same dominant topic for each model. In the best performing LDA models, the testing project should belong to a team with projects with the same functionality (Mob, Spotify) and the team should not have many other projects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "32BZZFlFhpzv"
   },
   "outputs": [],
   "source": [
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and their titles for the use cases 1 - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/anast/Desktop/Thesis/MachineLearning/datasetTitles.txt') as t:\n",
    "    titles = t.read().splitlines()\n",
    "\n",
    "# Use cases 1 & 3\n",
    "with open(\"C:/Users/anast/Desktop/Thesis/MachineLearning/Ontology/DatasetOntology/all.txt\") as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "# Use cases 2 & 4\n",
    "# with open(\"C:/Users/anast/Desktop/Thesis/MachineLearning/Data/datasetProjects.txt\") as f:\n",
    "#     data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and their titles for the use cases 5 - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('C:/Users/anast/Desktop/Thesis/MachineLearning/Th-Ur-Titles.txt') as t:\n",
    "#     titles = t.read().splitlines()\n",
    "\n",
    "# # Use Cases 5 & 7\n",
    "# with open(\"C:/Users/anast/Desktop/Thesis/MachineLearning/Ontology/DatasetOntology/Th-Ur-all.txt\") as f:\n",
    "#     data = f.read().splitlines()\n",
    "\n",
    "# # Use Cases 6 & 8\n",
    "# with open(\"C:/Users/anast/Desktop/Thesis/MachineLearning/Data/Th-Ur-Projects.txt\") as f:\n",
    "#     data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of data\n",
    "- Exclude the words of common functioanallity according to the use cases 3,4,7,8\n",
    "- Clean from numbers, punctuation and stop words\n",
    "- Lemmatize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPhTZv-0howO",
    "outputId": "889e4765-b97e-4f5f-b711-ee0633282200"
   },
   "outputs": [],
   "source": [
    "outputPath = \"C:/Users/anast/Desktop/Thesis/LDA/ResultsAll/\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "exclude = []\n",
    "rules = pd.read_csv('C:/Users/anast/Desktop/Results/results-all1.csv')\n",
    "rules = rules[(rules['Support']>0.2)][['Left Hand Side', 'Right Hand Side']]\n",
    "exclude.extend(rules['Left Hand Side'].tolist())\n",
    "exclude.extend(rules['Right Hand Side'].tolist())\n",
    "exclude = list(dict.fromkeys(exclude))\n",
    "exclude.extend(['datum', 'administrator', 'log', 'know', 'able', 'ability'])\n",
    "# exclude = []\n",
    "exclude.extend(['able', 'ability'])\n",
    "# exclude = []\n",
    "\n",
    "# Clean the data from numbers, punctuation and stop words\n",
    "clean_corpus = []\n",
    "for line in data:\n",
    "    doc = nlp(line)\n",
    "    cleanData = []\n",
    "    for token in doc:\n",
    "        if not token.is_alpha:\n",
    "            continue\n",
    "        elif token.is_stop:\n",
    "            continue\n",
    "        elif token.pos_ == \"PUNCT\":\n",
    "            continue\n",
    "        elif token.text in exclude:\n",
    "            continue\n",
    "        elif token.lemma_ in exclude:\n",
    "            continue \n",
    "        else:\n",
    "            cleanData.append(token.lemma_)\n",
    "    clean_corpus.append(cleanData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the bigram and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(clean_corpus, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[clean_corpus], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "texts = [bigram_mod[doc] for doc in clean_corpus]\n",
    "texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "data_ready = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jZt9O1cC3FBU"
   },
   "outputs": [],
   "source": [
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ks7oRweEjC2E"
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LDA models for different number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_iter = [14,3,7,4,6,19,2,18,10,8]\n",
    "topics_iter.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dKXnpJwsi33t"
   },
   "outputs": [],
   "source": [
    "for num_topics in topics_iter:\n",
    "\n",
    "  # Create Dictionary\n",
    "  id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "  # Create Corpus: Term Document Frequency\n",
    "  corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "  # Build LDA model\n",
    "  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=num_topics,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=10,\n",
    "                                            passes=10,\n",
    "                                            alpha='symmetric',\n",
    "                                            iterations=100,\n",
    "                                            per_word_topics=True)\n",
    "  if not os.path.exists(f'{outputPath}{num_topics}/'):      \n",
    "            os.makedirs(f'{outputPath}{num_topics}/')\n",
    "  lda_model.save(f\"{outputPath}{num_topics}/ldamodel\")\n",
    "\n",
    "  #################################\n",
    "  df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "  # Format\n",
    "  df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "  df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "  \n",
    "  df_dominant_topic.to_csv(f\"{outputPath}{num_topics}/dominant_topics.csv\", index = None)\n",
    "\n",
    "\n",
    "\n",
    "  ################################\n",
    "  # Display setting to show more characters in column\n",
    "  pd.options.display.max_colwidth = 100\n",
    "\n",
    "  sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "  sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "  for i, grp in sent_topics_outdf_grpd:\n",
    "      sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet,\n",
    "                                              grp.sort_values(['Perc_Contribution'], ascending=False).head(1)],\n",
    "                                              axis=0)\n",
    "\n",
    "  # Reset Index\n",
    "  sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  # Format\n",
    "  sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
    "\n",
    "  # Show\n",
    "  sent_topics_sorteddf_mallet.head(10)\n",
    "\n",
    "\n",
    "  #%%\n",
    "\n",
    "  doc_lens = [len(d) for d in df_dominant_topic.Text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, preprocess and get the bigram/trigram model of the testing project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data \n",
    "with open(\"C:/Users/anast/Desktop/testDataLDA.txt\") as f:\n",
    "    testdata = f.read().splitlines()\n",
    "\n",
    "# Clean the data from numbers, punctuation and stop words\n",
    "clean_corpus_test = []\n",
    "for line in testdata:\n",
    "    doc = nlp(line)\n",
    "    cleanData = []\n",
    "    for token in doc:\n",
    "        if not token.is_alpha:\n",
    "            continue\n",
    "        elif token.is_stop:\n",
    "            continue\n",
    "        elif token.pos_ == \"PUNCT\":\n",
    "            continue\n",
    "        elif token.text in exclude:\n",
    "            continue\n",
    "        elif token.lemma_ in exclude:\n",
    "            continue \n",
    "        else:\n",
    "            cleanData.append(token.lemma_)\n",
    "    clean_corpus_test.append(cleanData)\n",
    "\n",
    "bigramTest = gensim.models.Phrases(clean_corpus_test, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigramTest = gensim.models.Phrases(bigramTest[clean_corpus_test], threshold=100)\n",
    "bigram_modTest = gensim.models.phrases.Phraser(bigramTest)\n",
    "trigram_modTest = gensim.models.phrases.Phraser(trigramTest)\n",
    "\n",
    "textsTest = [bigram_modTest[doc] for doc in clean_corpus_test]\n",
    "textsTest = [trigram_modTest[bigram_mod[doc]] for doc in textsTest]\n",
    "\n",
    "dataTest = textsTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find which is the dominant topic for the testing project, for every LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xx_4ao7aR75x",
    "outputId": "19d5825d-5f12-4e04-e649-c945e7a16a07",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num_topics in topics_iter:\n",
    "  lda_model_loaded = gensim.models.ldamodel.LdaModel.load(f\"{outputPath}{num_topics}/ldamodel\")\n",
    "  id2word_loades = lda_model_loaded.id2word\n",
    "  # Create Corpus: Term Document Frequency\n",
    "  test_corpus = [id2word_loades.doc2bow(text) for text in dataTest]\n",
    "  topic_vec = lda_model_loaded[test_corpus]\n",
    "\n",
    "  # # For test \n",
    "  df_topic_sents_keywords_test = format_topics_sentences(ldamodel=lda_model_loaded, corpus=test_corpus, texts=dataTest)\n",
    "\n",
    "  # Format\n",
    "  df_dominant_topic = df_topic_sents_keywords_test.reset_index()\n",
    "  df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "  df_dominant_topic.to_csv(f\"{outputPath}{num_topics}/test_topic.csv\", index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "b1p-SQCKL1AB",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>manager, repository, archivist, training, file, upload, detect, info, alfred, video</td>\n",
       "      <td>['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8854</td>\n",
       "      <td>product, notify, cancel, list, request, store, profile, personal, rate, training</td>\n",
       "      <td>['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.4722</td>\n",
       "      <td>member, product, staff, profile, store, list, event, choose, review, send</td>\n",
       "      <td>['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>file, team, personal, upload, test, exchange, coach, athlete, cancel, allow</td>\n",
       "      <td>['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4677</td>\n",
       "      <td>file, team, personal, archivist, test, exchange, upload, athlete, coach, allow</td>\n",
       "      <td>['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3653</td>\n",
       "      <td>product, staff, notify, cancel, event, send, report, review, request, store</td>\n",
       "      <td>['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3644</td>\n",
       "      <td>manager, post, set, book, comment, profile, report, list, rate, country</td>\n",
       "      <td>['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.2792</td>\n",
       "      <td>cancel, detect, info, alfred, book, olderperson, message, salon, device, smart</td>\n",
       "      <td>['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.8111</td>\n",
       "      <td>notify, set, profile, list, team, personal, page, manage, rate, report</td>\n",
       "      <td>['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.8105</td>\n",
       "      <td>notify, list, set, profile, team, personal, page, rate, report, manage</td>\n",
       "      <td>['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             0.0              0.6071   \n",
       "0            0             0.0              0.8854   \n",
       "0            0             3.0              0.4722   \n",
       "0            0             1.0              0.4700   \n",
       "0            0             1.0              0.4677   \n",
       "0            0             3.0              0.3653   \n",
       "0            0             2.0              0.3644   \n",
       "0            0             8.0              0.2792   \n",
       "0            0            15.0              0.8111   \n",
       "0            0            15.0              0.8105   \n",
       "\n",
       "                                                                              Keywords  \\\n",
       "0  manager, repository, archivist, training, file, upload, detect, info, alfred, video   \n",
       "0     product, notify, cancel, list, request, store, profile, personal, rate, training   \n",
       "0            member, product, staff, profile, store, list, event, choose, review, send   \n",
       "0          file, team, personal, upload, test, exchange, coach, athlete, cancel, allow   \n",
       "0       file, team, personal, archivist, test, exchange, upload, athlete, coach, allow   \n",
       "0          product, staff, notify, cancel, event, send, report, review, request, store   \n",
       "0              manager, post, set, book, comment, profile, report, list, rate, country   \n",
       "0       cancel, detect, info, alfred, book, olderperson, message, salon, device, smart   \n",
       "0               notify, set, profile, list, team, personal, page, manage, rate, report   \n",
       "0               notify, list, set, profile, team, personal, page, rate, report, manage   \n",
       "\n",
       "                                                                                                  Text  \\\n",
       "0  ['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...   \n",
       "0  ['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...   \n",
       "0  ['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...   \n",
       "0  ['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...   \n",
       "0  ['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...   \n",
       "0  ['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...   \n",
       "0  ['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...   \n",
       "0  ['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...   \n",
       "0  ['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...   \n",
       "0  ['email', 'password', 'song', 'playlist', 'personal', 'info', 'profile', 'playlist', 'follow', '...   \n",
       "\n",
       "   topics  \n",
       "0       2  \n",
       "0       3  \n",
       "0       4  \n",
       "0       6  \n",
       "0       7  \n",
       "0       8  \n",
       "0      10  \n",
       "0      14  \n",
       "0      18  \n",
       "0      19  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame()\n",
    "for num_topics in topics_iter:\n",
    "  dato = pd.read_csv(f\"{outputPath}{num_topics}/test_topic.csv\")\n",
    "  result = result.append(dato)\n",
    "result['topics'] = topics_iter\n",
    "result.to_csv(f\"{outputPath}testResults.csv\", index = None)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find which other documents have the same dominant topic as the testing project, for every LDA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Topics: 2 ----------\n",
      "Test Data Topic: 0\n",
      "2: Car Gossip\n",
      "5: GiftCase\n",
      "6: HelpMe\n",
      "7: HotSpotManagement\n",
      "10: ProjectLibrary\n",
      "11: ProjectMedical\n",
      "20: ShrimpShip\n",
      "24: Software Patterns\n",
      "25: Taraxacum\n",
      "28: WikiPres\n",
      "41: g16-mis\n",
      "43: g18-neurohub\n",
      "44: g19-alfred\n",
      "45: g21-badcamp\n",
      "46: g22-rdadmp\n",
      "47: g23-archivesspace\n",
      "48: g24-unibath\n",
      "49: g25-duraspace\n",
      "50: g26-racdam\n",
      "52: g28-zooniverse\n",
      "54: RSE2020-THMMY\n",
      "56: RSE2020-Youtube\n",
      "57: RSE2020-Youtube2\n",
      "58: RSE2020-Shazam\n",
      "61: RSE2020-Twitter\n",
      "62: RSE2020-Dropbox\n",
      "63: DiabetesProject\n",
      "64: Project mob (team 20)\n",
      "69: Healthicle\n",
      "71: Shotgun!\n",
      "73: eSoula\n",
      "80: Google Translate\n",
      "83: Stixoiman\n",
      "84: YouTube\n",
      "85: Pinterest\n",
      "87: Tsagidi project (SPOTIFY APP)\n",
      "90: Tsagidi Project NodeRed\n",
      "91: Tsolakis Project Nodered\n",
      "92: Mikrouli project Nodered\n",
      "93: Gionavis project Nodered\n",
      "94: Patsika project Nodered\n",
      "95: Nastos project NodeRed\n",
      "96: Kalpakidis project Nodered\n",
      "97: Alexiou project Nodered\n",
      "100: SE 2021 Project - 2\n",
      "101: LSP Management\n",
      "115: Book Warehouse\n",
      "117: Rapix\n",
      "122: Safe Swim\n",
      "\n",
      "\n",
      "---------- Topics: 3 ----------\n",
      "Test Data Topic: 0\n",
      "1: BuySafe\n",
      "5: GiftCase\n",
      "7: HotSpotManagement\n",
      "10: ProjectLibrary\n",
      "11: ProjectMedical\n",
      "12: ProjectNews\n",
      "14: ProjectPlay\n",
      "15: ProjectStore\n",
      "21: SmartCar\n",
      "22: SmartCart\n",
      "25: Taraxacum\n",
      "27: TheOthers\n",
      "30: dataset 1\n",
      "44: g19-alfred\n",
      "52: g28-zooniverse\n",
      "53: Erasmus2020\n",
      "55: RSE2020-Gmail\n",
      "56: RSE2020-Youtube\n",
      "57: RSE2020-Youtube2\n",
      "58: RSE2020-Shazam\n",
      "60: RSE2020-efood\n",
      "63: DiabetesProject\n",
      "64: Project mob (team 20)\n",
      "65: Tourista\n",
      "66: Fitness box\n",
      "68: Foodman\n",
      "69: Healthicle\n",
      "70: e-Med\n",
      "71: Shotgun!\n",
      "73: eSoula\n",
      "75: ViruSeeker\n",
      "76: Ether\n",
      "77: What can you do?\n",
      "79: Whatsapp\n",
      "80: Google Translate\n",
      "82: E-Banking\n",
      "84: YouTube\n",
      "85: Pinterest\n",
      "87: Tsagidi project (SPOTIFY APP)\n",
      "89: Eshop demo project\n",
      "90: Tsagidi Project NodeRed\n",
      "91: Tsolakis Project Nodered\n",
      "92: Mikrouli project Nodered\n",
      "93: Gionavis project Nodered\n",
      "94: Patsika project Nodered\n",
      "95: Nastos project NodeRed\n",
      "96: Kalpakidis project Nodered\n",
      "97: Alexiou project Nodered\n",
      "98: SE2021 demo project\n",
      "100: SE 2021 Project - 2\n",
      "103: SE 2021 Project - 5\n",
      "105: SE 2021 Project - 7\n",
      "106: Self-Checkout Terminal\n",
      "108: weFeed\n",
      "112: SE 2021 Project - 15\n",
      "113: SE 2021 Project - 16\n",
      "115: Book Warehouse\n",
      "116: BookLand\n",
      "117: Rapix\n",
      "119: Travelink\n",
      "120: SE 2021 Project - 24\n",
      "121: SE 2021 Project - 25\n",
      "122: Safe Swim\n",
      "\n",
      "\n",
      "---------- Topics: 4 ----------\n",
      "Test Data Topic: 3\n",
      "1: BuySafe\n",
      "5: GiftCase\n",
      "9: Open Museum\n",
      "10: ProjectLibrary\n",
      "12: ProjectNews\n",
      "14: ProjectPlay\n",
      "15: ProjectStore\n",
      "16: RestMaps\n",
      "18: SCaseReqs\n",
      "19: Search4Yummy\n",
      "22: SmartCart\n",
      "27: TheOthers\n",
      "30: dataset 1\n",
      "32: g03-loudoun\n",
      "33: g04-recycling\n",
      "36: g10-scrumalliance\n",
      "51: g27-culrepo\n",
      "53: Erasmus2020\n",
      "55: RSE2020-Gmail\n",
      "60: RSE2020-efood\n",
      "62: RSE2020-Dropbox\n",
      "65: Tourista\n",
      "68: Foodman\n",
      "70: e-Med\n",
      "71: Shotgun!\n",
      "72: myDimos\n",
      "73: eSoula\n",
      "75: ViruSeeker\n",
      "76: Ether\n",
      "77: What can you do?\n",
      "79: Whatsapp\n",
      "81: Efood\n",
      "82: E-Banking\n",
      "85: Pinterest\n",
      "89: Eshop demo project\n",
      "98: SE2021 demo project\n",
      "100: SE 2021 Project - 2\n",
      "101: LSP Management\n",
      "102: Proaid - Team 7\n",
      "104: SE 2021 Project - 6\n",
      "105: SE 2021 Project - 7\n",
      "106: Self-Checkout Terminal\n",
      "108: weFeed\n",
      "110: NewsPub\n",
      "111: SE 2021 Project - 14\n",
      "112: SE 2021 Project - 15\n",
      "113: SE 2021 Project - 16\n",
      "116: BookLand\n",
      "117: Rapix\n",
      "119: Travelink\n",
      "121: SE 2021 Project - 25\n",
      "\n",
      "\n",
      "---------- Topics: 6 ----------\n",
      "Test Data Topic: 1\n",
      "4: EasyTV\n",
      "8: Lets Help Bo\n",
      "43: g18-neurohub\n",
      "50: g26-racdam\n",
      "51: g27-culrepo\n",
      "52: g28-zooniverse\n",
      "54: RSE2020-THMMY\n",
      "58: RSE2020-Shazam\n",
      "62: RSE2020-Dropbox\n",
      "64: Project mob (team 20)\n",
      "87: Tsagidi project (SPOTIFY APP)\n",
      "122: Safe Swim\n",
      "\n",
      "\n",
      "---------- Topics: 7 ----------\n",
      "Test Data Topic: 1\n",
      "4: EasyTV\n",
      "28: WikiPres\n",
      "43: g18-neurohub\n",
      "47: g23-archivesspace\n",
      "50: g26-racdam\n",
      "52: g28-zooniverse\n",
      "54: RSE2020-THMMY\n",
      "58: RSE2020-Shazam\n",
      "64: Project mob (team 20)\n",
      "80: Google Translate\n",
      "87: Tsagidi project (SPOTIFY APP)\n",
      "122: Safe Swim\n",
      "\n",
      "\n",
      "---------- Topics: 8 ----------\n",
      "Test Data Topic: 3\n",
      "1: BuySafe\n",
      "9: Open Museum\n",
      "22: SmartCart\n",
      "27: TheOthers\n",
      "30: dataset 1\n",
      "32: g03-loudoun\n",
      "53: Erasmus2020\n",
      "60: RSE2020-efood\n",
      "64: Project mob (team 20)\n",
      "66: Fitness box\n",
      "70: e-Med\n",
      "73: eSoula\n",
      "82: E-Banking\n",
      "89: Eshop demo project\n",
      "98: SE2021 demo project\n",
      "99: WalkMe\n",
      "100: SE 2021 Project - 2\n",
      "102: Proaid - Team 7\n",
      "103: SE 2021 Project - 5\n",
      "105: SE 2021 Project - 7\n",
      "106: Self-Checkout Terminal\n",
      "108: weFeed\n",
      "109: Walkio\n",
      "112: SE 2021 Project - 15\n",
      "116: BookLand\n",
      "117: Rapix\n",
      "\n",
      "\n",
      "---------- Topics: 10 ----------\n",
      "Test Data Topic: 2\n",
      "0: Auctioneer System\n",
      "13: ProjectPhoto\n",
      "16: RestMaps\n",
      "20: ShrimpShip\n",
      "24: Software Patterns\n",
      "45: g21-badcamp\n",
      "55: RSE2020-Gmail\n",
      "59: RSE2020-Blog\n",
      "61: RSE2020-Twitter\n",
      "66: Fitness box\n",
      "75: ViruSeeker\n",
      "80: Google Translate\n",
      "82: E-Banking\n",
      "85: Pinterest\n",
      "96: Kalpakidis project Nodered\n",
      "98: SE2021 demo project\n",
      "99: WalkMe\n",
      "104: SE 2021 Project - 6\n",
      "110: NewsPub\n",
      "113: SE 2021 Project - 16\n",
      "114: SE 2021 Project - 17\n",
      "115: Book Warehouse\n",
      "116: BookLand\n",
      "117: Rapix\n",
      "119: Travelink\n",
      "\n",
      "\n",
      "---------- Topics: 14 ----------\n",
      "Test Data Topic: 8\n",
      "2: Car Gossip\n",
      "44: g19-alfred\n",
      "64: Project mob (team 20)\n",
      "66: Fitness box\n",
      "87: Tsagidi project (SPOTIFY APP)\n",
      "91: Tsolakis Project Nodered\n",
      "92: Mikrouli project Nodered\n",
      "93: Gionavis project Nodered\n",
      "94: Patsika project Nodered\n",
      "95: Nastos project NodeRed\n",
      "97: Alexiou project Nodered\n",
      "115: Book Warehouse\n",
      "\n",
      "\n",
      "---------- Topics: 18 ----------\n",
      "Test Data Topic: 15\n",
      "0: Auctioneer System\n",
      "2: Car Gossip\n",
      "4: EasyTV\n",
      "12: ProjectNews\n",
      "13: ProjectPhoto\n",
      "14: ProjectPlay\n",
      "15: ProjectStore\n",
      "17: Restmarks\n",
      "20: ShrimpShip\n",
      "22: SmartCart\n",
      "24: Software Patterns\n",
      "27: TheOthers\n",
      "28: WikiPres\n",
      "29: WOQ\n",
      "30: dataset 1\n",
      "31: g02-federalspending\n",
      "33: g04-recycling\n",
      "36: g10-scrumalliance\n",
      "37: g11-nsf\n",
      "38: g12-camperplus\n",
      "39: g13-planningpoker\n",
      "45: g21-badcamp\n",
      "52: g28-zooniverse\n",
      "54: RSE2020-THMMY\n",
      "55: RSE2020-Gmail\n",
      "56: RSE2020-Youtube\n",
      "57: RSE2020-Youtube2\n",
      "58: RSE2020-Shazam\n",
      "59: RSE2020-Blog\n",
      "60: RSE2020-efood\n",
      "61: RSE2020-Twitter\n",
      "63: DiabetesProject\n",
      "64: Project mob (team 20)\n",
      "66: Fitness box\n",
      "68: Foodman\n",
      "69: Healthicle\n",
      "70: e-Med\n",
      "71: Shotgun!\n",
      "72: myDimos\n",
      "74: VICK\n",
      "75: ViruSeeker\n",
      "77: What can you do?\n",
      "78: Relief app\n",
      "79: Whatsapp\n",
      "80: Google Translate\n",
      "81: Efood\n",
      "82: E-Banking\n",
      "83: Stixoiman\n",
      "85: Pinterest\n",
      "87: Tsagidi project (SPOTIFY APP)\n",
      "92: Mikrouli project Nodered\n",
      "94: Patsika project Nodered\n",
      "95: Nastos project NodeRed\n",
      "96: Kalpakidis project Nodered\n",
      "98: SE2021 demo project\n",
      "99: WalkMe\n",
      "102: Proaid - Team 7\n",
      "103: SE 2021 Project - 5\n",
      "104: SE 2021 Project - 6\n",
      "105: SE 2021 Project - 7\n",
      "106: Self-Checkout Terminal\n",
      "108: weFeed\n",
      "111: SE 2021 Project - 14\n",
      "112: SE 2021 Project - 15\n",
      "113: SE 2021 Project - 16\n",
      "114: SE 2021 Project - 17\n",
      "115: Book Warehouse\n",
      "116: BookLand\n",
      "117: Rapix\n",
      "118: EVPoint-SE 2021 Project - 21\n",
      "119: Travelink\n",
      "120: SE 2021 Project - 24\n",
      "122: Safe Swim\n",
      "\n",
      "\n",
      "---------- Topics: 19 ----------\n",
      "Test Data Topic: 15\n",
      "0: Auctioneer System\n",
      "2: Car Gossip\n",
      "4: EasyTV\n",
      "12: ProjectNews\n",
      "13: ProjectPhoto\n",
      "14: ProjectPlay\n",
      "15: ProjectStore\n",
      "17: Restmarks\n",
      "20: ShrimpShip\n",
      "22: SmartCart\n",
      "23: SmartGo\n",
      "24: Software Patterns\n",
      "26: TheMatrix\n",
      "27: TheOthers\n",
      "30: dataset 1\n",
      "31: g02-federalspending\n",
      "33: g04-recycling\n",
      "36: g10-scrumalliance\n",
      "37: g11-nsf\n",
      "38: g12-camperplus\n",
      "39: g13-planningpoker\n",
      "40: g14-datahub\n",
      "41: g16-mis\n",
      "43: g18-neurohub\n",
      "44: g19-alfred\n",
      "45: g21-badcamp\n",
      "48: g24-unibath\n",
      "54: RSE2020-THMMY\n",
      "56: RSE2020-Youtube\n",
      "57: RSE2020-Youtube2\n",
      "58: RSE2020-Shazam\n",
      "59: RSE2020-Blog\n",
      "60: RSE2020-efood\n",
      "61: RSE2020-Twitter\n",
      "63: DiabetesProject\n",
      "64: Project mob (team 20)\n",
      "66: Fitness box\n",
      "68: Foodman\n",
      "69: Healthicle\n",
      "70: e-Med\n",
      "71: Shotgun!\n",
      "72: myDimos\n",
      "74: VICK\n",
      "75: ViruSeeker\n",
      "77: What can you do?\n",
      "78: Relief app\n",
      "79: Whatsapp\n",
      "80: Google Translate\n",
      "81: Efood\n",
      "82: E-Banking\n",
      "83: Stixoiman\n",
      "84: YouTube\n",
      "85: Pinterest\n",
      "87: Tsagidi project (SPOTIFY APP)\n",
      "92: Mikrouli project Nodered\n",
      "94: Patsika project Nodered\n",
      "95: Nastos project NodeRed\n",
      "96: Kalpakidis project Nodered\n",
      "97: Alexiou project Nodered\n",
      "98: SE2021 demo project\n",
      "99: WalkMe\n",
      "102: Proaid - Team 7\n",
      "103: SE 2021 Project - 5\n",
      "104: SE 2021 Project - 6\n",
      "105: SE 2021 Project - 7\n",
      "106: Self-Checkout Terminal\n",
      "108: weFeed\n",
      "111: SE 2021 Project - 14\n",
      "112: SE 2021 Project - 15\n",
      "113: SE 2021 Project - 16\n",
      "114: SE 2021 Project - 17\n",
      "115: Book Warehouse\n",
      "116: BookLand\n",
      "117: Rapix\n",
      "118: EVPoint-SE 2021 Project - 21\n",
      "119: Travelink\n",
      "120: SE 2021 Project - 24\n",
      "122: Safe Swim\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, num_topics in enumerate(topics_iter):\n",
    "  dominant_topics = pd.read_csv(f\"{outputPath}{num_topics}/dominant_topics.csv\", header = 0)\n",
    "  print(\"---------- Topics: \" + str(num_topics) + \" ----------\")\n",
    "  topic = result.iloc[index, 1]\n",
    "  print(\"Test Data Topic: \" + str(int(topic)))\n",
    "  for j in range(dominant_topics.__len__()):\n",
    "    if topic == int(dominant_topics.iloc[j, 1]):\n",
    "      print(str(j) + \": \" + titles[j]) \n",
    "  print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lda_topics_iterations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
